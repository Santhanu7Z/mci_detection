# -*- coding: utf-8 -*-
"""train.py (Pre-trained Mamba)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j_c9hdVmLN7xpE7QiVDeKXRD8fgWbx6d
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import pandas as pd
import whisper
from transformers import (
    AutoTokenizer,
    AutoModel,
    TrainingArguments,
    Trainer,
    PreTrainedModel,
    MambaConfig,
)
from transformers.modeling_outputs import SequenceClassifierOutput
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
import numpy as np
import argparse
from tqdm import tqdm
import math
from huggingface_hub import hf_hub_download
import json # Import json library to save history

# --- PURE PYTORCH MAMBA IMPLEMENTATION ---
# This compatible architecture will serve as the "body" for our "transplant".

class MambaBlock(nn.Module):
    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = self.expand * self.d_model
        self.dt_rank = math.ceil(self.d_model / 16)

        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=False)
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner, out_channels=self.d_inner,
            kernel_size=d_conv, bias=True, groups=self.d_inner, padding=d_conv - 1
        )
        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + self.d_state * 2, bias=False)
        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)

        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)
        self.A_log = nn.Parameter(torch.log(A))
        self.D = nn.Parameter(torch.ones(self.d_inner))
        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=False)

    def forward(self, x):
        b, l, d = x.shape
        x_and_res = self.in_proj(x)
        x, res = x_and_res.split(split_size=[self.d_inner, self.d_inner], dim=-1)
        x = x.transpose(1, 2)
        x = self.conv1d(x)[:, :, :l]
        x = x.transpose(1, 2)
        x = nn.functional.silu(x)
        x_dbl = self.x_proj(x)
        dt, B, C = x_dbl.split(split_size=[self.dt_rank, self.d_state, self.d_state], dim=-1)
        delta = nn.functional.softplus(self.dt_proj(dt))
        A = -torch.exp(self.A_log.float())
        y = self.selective_scan(x, delta, A, B, C)
        y = y * nn.functional.silu(res)
        return self.out_proj(y)

    def selective_scan(self, u, delta, A, B, C):
        b, l, d_in = u.shape
        n = A.shape[1]
        deltaA = torch.exp(delta.unsqueeze(-1) * A)
        deltaB_u = (delta * u).unsqueeze(-1) * B.unsqueeze(2)
        h = torch.zeros(b, d_in, n, device=u.device, dtype=torch.float32)
        ys = []
        for i in range(l):
            h = deltaA[:, i] * h + deltaB_u[:, i]
            y = (h @ C[:, i].unsqueeze(-1)).squeeze(-1)
            ys.append(y)
        return torch.stack(ys, dim=1)

class MambaForClassification(nn.Module):
    def __init__(self, d_model, n_layer, vocab_size, num_labels=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.layers = nn.ModuleList([MambaBlock(d_model) for _ in range(n_layer)])
        self.norm = nn.LayerNorm(d_model)
        self.classifier = nn.Linear(d_model, num_labels)

    def forward(self, input_ids):
        x = self.embedding(input_ids)
        for layer in self.layers:
            x = layer(x)
        x = self.norm(x)
        logits = self.classifier(x[:, -1, :])
        return logits

# --- Data Loading & Processing ---
def load_and_transcribe_data(metadata_path, whisper_model_size):
    print("Loading metadata...")
    df = pd.read_csv(metadata_path)
    print(f"Loading Whisper model: {whisper_model_size}...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    whisper_model = whisper.load_model(whisper_model_size, device=device)
    transcriptions = []
    print("Transcribing audio files...")
    for audio_path in tqdm(df['audio_path'], desc="Transcribing"):
        if not os.path.exists(audio_path):
            transcriptions.append("")
            continue
        result = whisper_model.transcribe(audio_path, fp16=torch.cuda.is_available())
        transcriptions.append(result['text'])
    df['transcription'] = transcriptions
    df = df[df['transcription'].str.strip() != ''].reset_index(drop=True)
    return df

class MCIDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    def __len__(self):
        return len(self.encodings['input_ids'])

# --- Main Training Function ---
def main(args):
    PROCESSED_DATA_PATH = "processed_data/metadata.csv"
    MODEL_OUTPUT_DIR = "trained_model_mamba_pretrained"
    TOKENIZER_ID = "EleutherAI/gpt-neox-20b"

    df = load_and_transcribe_data(PROCESSED_DATA_PATH, args.whisper_model)
    df['label_id'] = df['label'].apply(lambda x: 1 if x == 'Dementia' else 0)
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label_id'])

    print(f"Loading tokenizer: {TOKENIZER_ID}...")
    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_ID)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("Tokenizing datasets...")
    max_length = 2048
    train_encodings = tokenizer(train_df['transcription'].tolist(), truncation=True, padding=True, max_length=max_length)
    test_encodings = tokenizer(test_df['transcription'].tolist(), truncation=True, padding=True, max_length=max_length)

    train_dataset = MCIDataset(train_encodings, train_df['label_id'].tolist())
    test_dataset = MCIDataset(test_encodings, test_df['label_id'].tolist())

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    print("Initializing Mamba model architecture...")
    model = MambaForClassification(
        d_model=768,
        n_layer=24,
        vocab_size=len(tokenizer)
    )

    # --- THE MODEL TRANSPLANT ---
    print(f"Downloading pre-trained weights from {args.mamba_model}...")
    model_weights_path = hf_hub_download(repo_id=args.mamba_model, filename="pytorch_model.bin")
    pretrained_weights = torch.load(model_weights_path, map_location=device)

    model.load_state_dict(pretrained_weights, strict=False)
    print("Pre-trained weights successfully loaded into custom Mamba model.")
    # ---------------------------

    model.to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    criterion = nn.CrossEntropyLoss()

    # --- NEW: Store history ---
    history = {'train_loss': [], 'val_accuracy': [], 'val_f1': []}

    for epoch in range(args.epochs):
        model.train()
        total_loss = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{args.epochs}")
        for batch in progress_bar:
            optimizer.zero_grad()
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)

            logits = model(input_ids)
            loss = criterion(logits, labels)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

        avg_train_loss = total_loss / len(train_loader)
        history['train_loss'].append(avg_train_loss)
        print(f"Epoch {epoch+1} Average Training Loss: {avg_train_loss:.4f}")

        model.eval()
        all_preds, all_labels = [], []
        with torch.no_grad():
            for batch in test_loader:
                input_ids = batch['input_ids'].to(device)
                labels = batch['labels'].to(device)
                logits = model(input_ids)
                preds = torch.argmax(logits, dim=1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

        acc = accuracy_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds, average='weighted')
        history['val_accuracy'].append(acc)
        history['val_f1'].append(f1)
        print(f"Epoch {epoch+1} - Validation Accuracy: {acc:.4f}, F1 Score: {f1:.4f}")

    print(f"Training complete. Saving model to {MODEL_OUTPUT_DIR}")
    os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(MODEL_OUTPUT_DIR, "pytorch_model.bin"))
    tokenizer.save_pretrained(MODEL_OUTPUT_DIR)

    # --- NEW: Save the history log to a file ---
    history_path = os.path.join(MODEL_OUTPUT_DIR, 'training_history.json')
    with open(history_path, 'w') as f:
        json.dump(history, f, indent=4)
    print(f"Training history saved to {history_path}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Train a Mamba model for MCI detection.")
    parser.add_argument('--epochs', type=int, default=3, help='Number of training epochs.')
    parser.add_argument('--batch_size', type=int, default=1, help='Training and evaluation batch size.')
    parser.add_argument('--whisper_model', type=str, default='tiny.en', help='Whisper model size.')
    parser.add_argument('--mamba_model', type=str, default='state-spaces/mamba-130m', help='Hugging Face ID of the Mamba model to download weights from.')

    args = parser.parse_args()
    main(args)