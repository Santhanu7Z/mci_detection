# -*- coding: utf-8 -*-
"""predict.py (Pre-trained Mamba)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13ezZxpvdtcwdzKX3hn_6TU073JSswCrz
"""

import torch
import torch.nn as nn
import whisper
from transformers import AutoTokenizer
import argparse
import os
import math

# --- PURE PYTORCH MAMBA IMPLEMENTATION ---
# This must be the EXACT same class definition as in train.py to load the model correctly.

class MambaBlock(nn.Module):
    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = self.expand * self.d_model
        self.dt_rank = math.ceil(self.d_model / 16)

        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=False)
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner, out_channels=self.d_inner,
            kernel_size=d_conv, bias=True, groups=self.d_inner, padding=d_conv - 1
        )
        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + self.d_state * 2, bias=False)
        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)

        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)
        self.A_log = nn.Parameter(torch.log(A))
        self.D = nn.Parameter(torch.ones(self.d_inner))
        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=False)

    def forward(self, x):
        b, l, d = x.shape
        x_and_res = self.in_proj(x)
        x, res = x_and_res.split(split_size=[self.d_inner, self.d_inner], dim=-1)
        x = x.transpose(1, 2)
        x = self.conv1d(x)[:, :, :l]
        x = x.transpose(1, 2)
        x = nn.functional.silu(x)
        x_dbl = self.x_proj(x)
        dt, B, C = x_dbl.split(split_size=[self.dt_rank, self.d_state, self.d_state], dim=-1)
        delta = nn.functional.softplus(self.dt_proj(dt))
        A = -torch.exp(self.A_log.float())
        y = self.selective_scan(x, delta, A, B, C)
        y = y * nn.functional.silu(res)
        return self.out_proj(y)

    def selective_scan(self, u, delta, A, B, C):
        b, l, d_in = u.shape
        n = A.shape[1]
        deltaA = torch.exp(delta.unsqueeze(-1) * A)
        deltaB_u = (delta * u).unsqueeze(-1) * B.unsqueeze(2)
        h = torch.zeros(b, d_in, n, device=u.device, dtype=torch.float32)
        ys = []
        for i in range(l):
            h = deltaA[:, i] * h + deltaB_u[:, i]
            y = (h @ C[:, i].unsqueeze(-1)).squeeze(-1)
            ys.append(y)
        return torch.stack(ys, dim=1)

class MambaForClassification(nn.Module):
    def __init__(self, d_model, n_layer, vocab_size, num_labels=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.layers = nn.ModuleList([MambaBlock(d_model) for _ in range(n_layer)])
        self.norm = nn.LayerNorm(d_model)
        self.classifier = nn.Linear(d_model, num_labels)

    def forward(self, input_ids):
        x = self.embedding(input_ids)
        for layer in self.layers:
            x = layer(x)
        x = self.norm(x)
        logits = self.classifier(x[:, -1, :])
        return logits

# --- Configuration ---
MODEL_PATH = "trained_model_mamba_pretrained"
TOKENIZER_ID = "EleutherAI/gpt-neox-20b"

def predict_mci(audio_path, whisper_model_size):
    if not os.path.exists(audio_path):
        return "Error: Audio file not found.", 0.0
    if not os.path.exists(os.path.join(MODEL_PATH, "pytorch_model.bin")):
        return "Error: Trained model not found. Please run train.py first.", 0.0

    device = "cuda" if torch.cuda.is_available() else "cpu"

    print(f"Loading Whisper model ({whisper_model_size}) and transcribing...")
    whisper_model = whisper.load_model(whisper_model_size, device=device)
    result = whisper_model.transcribe(audio_path, fp16=torch.cuda.is_available())
    transcription = result['text']
    print(f"\nTranscription: {transcription}\n")

    print("Loading fine-tuned Mamba model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

    model = MambaForClassification(
        d_model=768,
        n_layer=24,
        vocab_size=len(tokenizer)
    )
    # Load the fine-tuned weights we saved
    model.load_state_dict(torch.load(os.path.join(MODEL_PATH, "pytorch_model.bin")))
    model.to(device)
    model.eval()

    print("Making prediction...")
    inputs = tokenizer(transcription, return_tensors="pt", truncation=True, padding=True, max_length=2048).to(device)

    with torch.no_grad():
        logits = model(inputs['input_ids'])
        probabilities = torch.softmax(logits, dim=-1)

    dementia_confidence = probabilities[0][1].item()
    predicted_class_id = torch.argmax(probabilities).item()

    prediction = "Dementia" if predicted_class_id == 1 else "Control"

    return prediction, dementia_confidence

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Predict MCI from an audio file.")
    parser.add_argument('audio_file', type=str, help="Path to the audio file to be analyzed.")
    parser.add_argument('--whisper_model', type=str, default='tiny.en', help='Whisper model size to use for transcription.')
    args = parser.parse_args()

    predicted_label, confidence = predict_mci(args.audio_file, args.whisper_model)

    if "Error" not in predicted_label:
        print(f"\n--- Prediction Result ---")
        print(f"Predicted Label: {predicted_label}")
        print(f"Confidence (Dementia): {confidence:.4f}")
    else:
        print(f"\n--- Error ---")
        print(predicted_label)