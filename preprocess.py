# -*- coding: utf-8 -*-
"""preprocess.py (Corrected)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FsC5UaboWQmIRfVu88RAYBEWw2DGIxpk
"""

import os
import re
import pandas as pd
from pydub import AudioSegment
from tqdm import tqdm
import soundfile as sf
import argparse

# This regex pattern is now designed to parse lines with the user-friendly
# %x_...% timestamp format, all on one line.
CHAT_LINE_PATTERN = re.compile(r'^\*PAR:\s*(.*?)\s*(%x_\d+_\d+%)$')

def parse_cha_file(file_path):
    """
    Parses a .cha transcript file to extract the participant's speech segments.

    Args:
        file_path (str): The path to the .cha file.

    Returns:
        list: A list of tuples, where each tuple contains the spoken text,
              the start time (ms), and the end time (ms) of a speech segment.
    """
    segments = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                match = CHAT_LINE_PATTERN.match(line.strip())
                if match:
                    # The timestamp is now in the format %x_startTime_endTime%
                    # We extract the start and end times in milliseconds.
                    text, timestamp = match.groups()
                    # A more robust way to extract numbers from the timestamp string
                    times_str = timestamp.replace('%x_', '').replace('%', '')
                    start_ms, end_ms = map(int, times_str.split('_'))
                    segments.append((text.strip(), start_ms, end_ms))
    except Exception as e:
        print(f"Error parsing file {file_path}: {e}")
    return segments

def process_audio_and_transcripts(base_path, output_path):
    """
    Main function to process the entire DementiaBank Pitt corpus.

    It iterates through 'Control' and 'Dementia' groups, finds audio/transcript
    pairs, extracts participant-only audio segments, and saves them to a
    new directory. It also creates a metadata CSV file.

    Args:
        base_path (str): The path to the raw DementiaBank Pitt data.
        output_path (str): The path where processed data will be saved.
    """
    metadata = []
    # Ensure output directories exist
    os.makedirs(os.path.join(output_path, 'audio'), exist_ok=True)

    groups = ['Control', 'Dementia']
    for group in groups:
        group_path = os.path.join(base_path, group)
        if not os.path.isdir(group_path):
            print(f"Warning: Directory not found for group '{group}'. Skipping.")
            continue

        all_files = os.listdir(group_path)
        # Filter for .cha files to avoid processing other file types
        cha_files = [f for f in all_files if f.endswith('.cha')]

        print(f"Processing group: {group}...")
        for cha_file_name in tqdm(cha_files, desc=f"Processing {group}"):
            participant_id = cha_file_name.split('.')[0]
            cha_path = os.path.join(group_path, cha_file_name)

            # Find the corresponding audio file (assuming .wav format)
            audio_file_name = f"{participant_id}.wav"
            audio_path = os.path.join(group_path, audio_file_name)

            if not os.path.exists(audio_path):
                print(f"Warning: Audio file not found for {participant_id}. Skipping.")
                continue

            # Parse the transcript to get speech segments
            segments = parse_cha_file(cha_path)
            if not segments:
                print(f"Warning: No participant segments found in {cha_file_name}. Skipping.")
                continue

            try:
                # Load the full audio file
                full_audio = AudioSegment.from_wav(audio_path)
                participant_audio = AudioSegment.empty()

                # Concatenate all speech segments from the participant
                for _, start_ms, end_ms in segments:
                    segment_audio = full_audio[start_ms:end_ms]
                    participant_audio += segment_audio

                # Save the processed audio file
                output_audio_filename = f"{participant_id}_participant.wav"
                output_audio_path = os.path.join(output_path, 'audio', output_audio_filename)

                # Export the concatenated audio to ensure a standard WAV format
                participant_audio.export(output_audio_path, format="wav")

                # Add record to our metadata list
                metadata.append({
                    'participant_id': participant_id,
                    'label': group,
                    'audio_path': output_audio_path
                })
            except Exception as e:
                print(f"Error processing audio for {participant_id}: {e}")

    # Save the metadata to a CSV file
    df = pd.DataFrame(metadata)
    df.to_csv(os.path.join(output_path, 'metadata.csv'), index=False)
    print(f"\nPreprocessing complete. Metadata saved to {os.path.join(output_path, 'metadata.csv')}")
    print(f"Processed audio saved in {os.path.join(output_path, 'audio')}")


if __name__ == '__main__':
    # Setup command-line argument parsing
    parser = argparse.ArgumentParser(description="Preprocess DementiaBank Pitt Corpus Data.")
    parser.add_argument(
        '--data_path',
        type=str,
        default='data/Pitt',
        help="Path to the raw DementiaBank Pitt data directory."
    )
    parser.add_argument(
        '--output_path',
        type=str,
        default='processed_data',
        help="Path to save the processed data and metadata.csv."
    )
    args = parser.parse_args()

    # Run the main processing function
    process_audio_and_transcripts(args.data_path, args.output_path)